{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import shutil\n",
    "from xlsxwriter import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' print(os.path.exists(path))\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class App:\n",
    "    def __init__(self,username,password,target_username,path='C:\\\\Users\\\\hebaa\\\\Documents\\web scraping with python beautifulsoup course implementation\\\\instphoto'):\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        self.username=username\n",
    "        self.password=password\n",
    "        self.target_username=target_username\n",
    "        self.path=path\n",
    "        self.driver=webdriver.Chrome('C:\\webdrivers\\chromedriver.exe')\n",
    "        self.main_url='https://www.instagram.com'\n",
    "        self.driver.get(self.main_url)\n",
    "        sleep(3)\n",
    "        self.log_in()\n",
    "        self.open_target_profile()\n",
    "        sleep(3)\n",
    "        self.scroll_down()\n",
    "        self.download_image()\n",
    "        sleep(3)\n",
    "        \n",
    "        self.driver.close()\n",
    "        \n",
    "    def toexcell(self,images,caption_pathhh):\n",
    "        #wb=Workbook(caption_pathhh +'captions.xlsx')\n",
    "        wb=Workbook(os.path.join(caption_pathhh,'captions.xlsx'))\n",
    "        ws=wb.add_worksheet()\n",
    "        row=0\n",
    "        ws.write(row,0,'img_name')\n",
    "        ws.write(row,1,'caption')\n",
    "        row+= 1\n",
    "        for index,image in enumerate(images):\n",
    "            filename='image_'+str(index)+'.jpg'\n",
    "            try:\n",
    "                caption=image['alt']\n",
    "            except:\n",
    "                caption='no caption exist'\n",
    "            ws.write(row,0,filename)\n",
    "            ws.write(row,1,caption)\n",
    "            row+= 1\n",
    "        \n",
    "        \n",
    "        wb.close()\n",
    "        \n",
    "    def download_caption(self,images):\n",
    "        captionpath_folder=os.path.join(self.path,'captions')\n",
    "        if not os.path.exists(captionpath_folder):\n",
    "            os.mkdir(captionpath_folder)\n",
    "        self.toexcell(images,captionpath_folder)\n",
    "        for index,image in enumerate(images):\n",
    "            try:\n",
    "                caption=image['alt']\n",
    "                print(image['alt'])\n",
    "            except:\n",
    "                print('there are no caption for this image')\n",
    "            caname='cation_'+str(index)+'.txt'\n",
    "            capath_file=os.path.join(captionpath_folder,caname)\n",
    "            with open(capath_file,'wb') as file:\n",
    "                file.write(str('link'+'\\n'+str(image['src'])+'\\n'+'caption'+caption).encode())\n",
    "                \n",
    "    def download_image(self):\n",
    "        soup=BeautifulSoup(self.driver.page_source,'lxml')\n",
    "        all_images=soup.find_all('img')\n",
    "        self.download_caption(all_images)\n",
    "        print('len of all images',len(all_images))\n",
    "        for index,image in enumerate(all_images):\n",
    "            filename='image_'+str(index)+'.jpg'\n",
    "            #imagepath=self.path+'/'+filename\n",
    "            imagepath=os.path.join(self.path,filename)\n",
    "            response=requests.get(image['src'],stream=True)\n",
    "            with open(imagepath,'wb') as file:\n",
    "                shutil.copyfileobj(response.raw,file)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def scroll_down(self):\n",
    "        no_of_posted=self.driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/header/section/ul/li[1]/span/span')\n",
    "        self.no_of_posted=int(str(no_of_posted.text).replace(',',''))\n",
    "        if self.no_of_posted>13:\n",
    "            no_of_scroll=int(self.no_of_posted/13)\n",
    "            print(no_of_scroll)\n",
    "            for value in range(no_of_scroll):\n",
    "                self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                sleep(3) \n",
    "            \n",
    "        \n",
    "    def open_target_profile(self):\n",
    "        self.driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[2]/input').send_keys(self.target_username)\n",
    "        #search_bar.submit()\n",
    "        self.driver.get(self.main_url+'/'+self.target_username+'/')\n",
    "        sleep(1)\n",
    "        \n",
    "    def skip_notification(self):\n",
    "        try:\n",
    "            skip_not=self.driver.find_element_by_xpath('/html/body/div[3]/div/div/div[3]/button[2]').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def log_in(self):\n",
    "        login_button=self.driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div[2]/p/a').click()\n",
    "        sleep(3)\n",
    "        username_input=self.driver.find_element_by_xpath('//input[@aria-label=\"Phone number, username, or email\"]')\n",
    "        username_input.send_keys(self.username)\n",
    "        password_input=self.driver.find_element_by_xpath('//input[@aria-label=\"Password\"]')\n",
    "        password_input.send_keys(self.password)\n",
    "        password_input.submit()\n",
    "        sleep(3)\n",
    "        self.skip_notification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "dataminer2060's profile picture\n",
      "No photo description available.\n",
      "Image may contain: outdoor\n",
      "Image may contain: outdoor\n",
      "Image may contain: 1 person, outdoor\n",
      "Image may contain: 1 person, tree, plant, outdoor, nature and water\n",
      "Image may contain: swimming\n",
      "Image may contain: 2 people, cat\n",
      "No photo description available.\n",
      "No photo description available.\n",
      "No photo description available.\n",
      "No photo description available.\n",
      "No photo description available.\n",
      "Milky Way\n",
      "Volcanic clouds\n",
      "Northern lights\n",
      "Foggy Morning\n",
      "Random click\n",
      "there are no caption for this image\n",
      "Night driving\n",
      "It's cold\n",
      "Cake 🎂\n",
      "Random\n",
      "#sunset #birds\n",
      "#nighttime\n",
      "Desert 🌵\n",
      "Aansu Lake 💧\n",
      "Good Morning 🌝\n",
      "Fog\n",
      "Sunset 🌅\n",
      "Sky 😎\n",
      "Normal day\n",
      "Kunhar 😍\n",
      "Memories 🌞\n",
      "View <3\n",
      "Mountains 😍😍\n",
      "😍😍😍\n",
      "len of all images 37\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    app=App('heba.abdelshafi','********','dataminer2060')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\webdrivers\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.instagram.com'+'/'+'natgeo'+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/ul/li[1]/a/span').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
